{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# chapter 5-1 データセットの取得と活用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Wikipediaのデータセットのダウンロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !wget https://dumps.wikimedia.org/jawiki/latest/jawiki-latest-pages-articles1.xml-p1p168815.bz2\n",
    "# Wikipediaのデータセット(ダンプファイル)の取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ! bzcat jawiki-latest-pages-articles1.xml-p1p168815.bz2 | less\n",
    "# 解凍せず中身を閲覧可能\n",
    "# j: 下スクロール, k: 上スクロール, q: 終了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !wget https://github.com/attardi/wikiextractor/raw/master/WikiExtractor.py\n",
    "# WikiExtractor.py スクリプトをインストール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !python WikiExtractor.py --no-templates -o articles -b 100M jawiki-latest-pages-articles1.xml-p1p168815.bz2\n",
    "# スクリプトを用いてダンプファイルのマークアップを取り除きテキストに変換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !tree articles/\n",
    "# ディレクトリの確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 自然言語処理技術を用いた頻出単語の抽出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !brew install mecab-ipadic\n",
    "# MeCabのインストール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !mecav -v\n",
    "# バージョンの確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !pip install mecab-python3\n",
    "# Python3対応バインディングのインストール(MeCab公式ではない。公式は未対応?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MeCabをPythonから使う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mecab_sample.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mecab_sample.py\n",
    "\n",
    "# MeCabをPythonから使う\n",
    "\n",
    "import MeCab\n",
    "\n",
    "tagger = MeCab.Tagger()\n",
    "tagger.parse('') # これは.parseToNode()の不具合を回避するのに必要\n",
    "\n",
    "# .parseToNode()で最初の形態素を表すNodeオブジェクトを取得する\n",
    "node = tagger.parseToNode('すもももももももものうち')\n",
    "\n",
    "while node:\n",
    "    # .surfaceは形態素の文字列、.featureは品詞などを含む文字列をそれぞれ表す\n",
    "    print(node.surface, node.feature)\n",
    "    node = node.next # .nextで次のNodeを取得する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " BOS/EOS,*,*,*,*,*,*,*,*\r\n",
      "すもも 名詞,一般,*,*,*,*,すもも,スモモ,スモモ\r\n",
      "も 助詞,係助詞,*,*,*,*,も,モ,モ\r\n",
      "もも 名詞,一般,*,*,*,*,もも,モモ,モモ\r\n",
      "も 助詞,係助詞,*,*,*,*,も,モ,モ\r\n",
      "もも 名詞,一般,*,*,*,*,もも,モモ,モモ\r\n",
      "の 助詞,連体化,*,*,*,*,の,ノ,ノ\r\n",
      "うち 名詞,非自立,副詞可能,*,*,*,うち,ウチ,ウチ\r\n",
      " BOS/EOS,*,*,*,*,*,*,*,*\r\n"
     ]
    }
   ],
   "source": [
    "!python mecab_sample.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 以下でもほぼ同様の結果が得られる\n",
    "# !echo すもももももももものうち | mecab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 文章から頻出単語を抽出する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Wikipediaの文章から頻出単語を抜き出す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing word_frequency.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile word_frequency.py\n",
    "\n",
    "# Wikipediaの文章から頻出単語を抜き出す\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from glob import glob\n",
    "from collections import Counter\n",
    "import MeCab\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    コマンドライン引数で指定したディレクトリ内のファイルを読み込んで頻出単語を抽出する\n",
    "    \"\"\"\n",
    "    \n",
    "    input_dir = sys.argv[1] # コマンドラインの第1引数でWikiExtractorの出力先のディレクトリを指定する\n",
    "    \n",
    "    tagger = MeCab.Tagger('')\n",
    "    tagger.parse('') # .parseToNode()の不具合を回避するのに必要\n",
    "    # 単語の頻度を格納するCounterオブジェクトを作成する\n",
    "    # Counterクラスはdictを継承しており、値としてキーの出現回数を保持する\n",
    "    frequency = Counter()\n",
    "    count_proccessed = 0\n",
    "    \n",
    "    # glob()でワイルドカードにマッチするファイルのリストを取得し、マッチしたすべてのファイルを処理する\n",
    "    for path in glob(os.path.join(input_dir, '*', 'wiki_*')):\n",
    "        print('Proccessing {0}...'.format(path), file=sys.stderr)\n",
    "        \n",
    "        with open(path) as file: # ファイルを開く\n",
    "            for content in iter_docs(file): # ファイル内の全記事について反復処理する\n",
    "                tokens = get_tokens(tagger, content) # ページから名詞のリストを取得する\n",
    "                # Counterのupdate()メソッドにリストなどの反復可能オブジェクトを指定すると、\n",
    "                # リストに含まれる値の出現回数を一度に増やせる\n",
    "                frequency.update(tokens)\n",
    "                \n",
    "                # 10,000件ごとに進捗を表示\n",
    "                count_proccessed += 1\n",
    "                if count_proccessed % 10000 == 0:\n",
    "                    print('{0} documents were proccessed.'.format(count_proccessed),\n",
    "                         file=sys.stderr)\n",
    "                    \n",
    "    # 全記事の処理が完了したら上位30件の名詞と出現回数を表示する\n",
    "    for token, count in frequency.most_common(30):\n",
    "        print(token, count)\n",
    "        \n",
    "        \n",
    "def iter_docs(file):\n",
    "    \"\"\"\n",
    "    ファイルオブジェクトを読み込んで、記事の中身(開始タグ<doc ...>と終了タグ</doc>の間のテキスト)を\n",
    "    順に返すジェネレーター関数\n",
    "    \"\"\"\n",
    "    \n",
    "    for line in file:\n",
    "        if line.startswith('<doc '):\n",
    "            buffer = [] # 開始タグが見つかったらバッファを初期化する\n",
    "        elif line.startswith('</doc>'):\n",
    "            # 終了タグが見つかったらバッファの中身を結合してyieldする\n",
    "            content = ''.join(buffer)\n",
    "            yield content\n",
    "        else:\n",
    "            buffer.append(line) # 開始タグ・終了タグ以外の業はバッファに追加する\n",
    "            \n",
    "            \n",
    "def get_tokens(tagger, content):\n",
    "    \"\"\"\n",
    "    文書内に出現した名詞のリストを取得する関数\n",
    "    \"\"\"\n",
    "    \n",
    "    tokens = [] # この記事で出現した名詞を格納するリスト\n",
    "    \n",
    "    node = tagger.parseToNode(content)\n",
    "    while node:\n",
    "        # node.featureはカンマで区切られた文字列なので、split() で分割して\n",
    "        # 最初の2項目をcategoryとsub_categoryに代入する\n",
    "        category, sub_category = node.feature.split(',')[:2]\n",
    "        # 固有名詞または一般名詞の場合のみtokensに追加する\n",
    "        if category == '名詞' and sub_category in ('固有名詞', '一般'):\n",
    "            tokens.append(node.surface)\n",
    "        node = node.next\n",
    "        \n",
    "    return tokens\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proccessing articles/AA/wiki_00...\n",
      "10000 documents were proccessed.\n",
      "Proccessing articles/AA/wiki_01...\n",
      "20000 documents were proccessed.\n",
      "30000 documents were proccessed.\n",
      "Proccessing articles/AA/wiki_02...\n",
      "40000 documents were proccessed.\n",
      "50000 documents were proccessed.\n",
      "Proccessing articles/AA/wiki_03...\n",
      "60000 documents were proccessed.\n",
      "70000 documents were proccessed.\n",
      "Proccessing articles/AA/wiki_04...\n",
      "80000 documents were proccessed.\n",
      "月 298912\n",
      "日本 130996\n",
      "時代 65654\n",
      "駅 51080\n",
      "世界 46833\n",
      "作品 44752\n",
      "番組 43670\n",
      "列車 43488\n",
      "昭和 42446\n",
      "東京 41921\n",
      "一般 38469\n",
      "地域 37079\n",
      "鉄道 37058\n",
      "平成 36889\n",
      "中心 36282\n",
      "アメリカ 36205\n",
      "ホーム 31774\n",
      "世紀 30718\n",
      "バス 30405\n",
      "大学 30271\n",
      "車両 29862\n",
      "間 29861\n",
      "路線 29207\n",
      "映画 28781\n",
      "他 28559\n",
      "学校 27057\n",
      "形 26538\n",
      "ドイツ 26472\n",
      "事業 26380\n",
      "テレビ 26332\n"
     ]
    }
   ],
   "source": [
    "!python word_frequency.py articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# chapter 5-2 APIによるデータの収集と活用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
