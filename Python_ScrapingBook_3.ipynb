{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# chapter 3-2 webページを簡単に取得する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r = requests.get('https://gihyo.jp/dp')\n",
    "# webページを取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(r)\n",
    "# get関数の戻り値はレスポンス型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.status_code\n",
    "# HTTPステータスコードを取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.headers['content-type']\n",
    "# HTTPヘッダーの辞書を取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.encoding\n",
    "# HTTPヘッダーから得られたエンコーディングを取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.text\n",
    "# str型にデコードしたレスポンスボディを取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.content\n",
    "# bytes型のレスポンスボディを取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get('http://weather.livedoor.com/forecast/webservice/json/v1?city=130010')\n",
    "# 東京の天気をjson形式で取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r = requests.post('http://httpbin.org/post', data={'key1': 'value1'})\n",
    "# POSTメソッドで送信"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get('http://httpbin.org/get',\n",
    "                headers={'user-agent': 'my-crawler/1.0 (+foo@example.com)'})\n",
    "# キーワード引数headersにdictで指定してリクエストにHTTPヘッダーを追加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get('https://api.github.com/user',\n",
    "                auth=('discocactus', '<password>'))\n",
    "# Basic認証のユーザー名とパスワードの組をキーワード引数authで指定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get('http://httpbin.org/get', params={'key1': 'value1'})\n",
    "# URLのパラメーターは引数paramsで指定することも可能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 複数のページを連続してクロールする場合は、Sessionオブジェクトを使うのが効果的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = requests.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.headers.update({'user-agent': 'my-crawler/1.0 (+foo@example.com)'})\n",
    "# HTTPヘッダーを複数のリクエストで使い回す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r = s.get('https://gihyo.jp/')\n",
    "# Sessionオブジェクトでもrequestsの様にget(), post()などのメソッドが使える"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r = s.get('https://gihyo.jp/dp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# chapter 3-3 HTMLのスクレイピング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lxmlによるスクレイピング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lxml.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tree = lxml.html.parse('index.html')\n",
    "# parse()関数でファイルパスを指定してパース\n",
    "# URLを指定することも可能だが細かい指定ができない"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tree = lxml.html.parse(urlopen('http://example.com/'))\n",
    "# ファイルオブジェクトを指定してパースすることも可能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "html = tree.getroot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fromstring()関数で文字列(strまたはbytes)をパースできる\n",
    "# ただし、encodingが指定されたXML宣言を含むstrをパースすると、ValueErrorが発生するので注意"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "html = lxml.html.fromstring('''\n",
    "    <html>\n",
    "    <head><title>八百屋オンライン</title></head>\n",
    "    <body>\n",
    "    <h1 id=\"main\">今日のくだもの</h1>\n",
    "    <ul>\n",
    "        <li>りんご</li>\n",
    "        <li class=\"featured\">みかん</li>\n",
    "        <li>ぶどう</li>\n",
    "    </ul>\n",
    "    </body>\n",
    "    </html>''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html.xpath('//li')\n",
    "# XPathにマッチする要素のリストを取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html.cssselect('li')\n",
    "# CSSセレクターにマッチする要素のリストを取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h1 = html.xpath('//h1')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1.tag\n",
    "# タグの名前"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1.text\n",
    "# 要素のテキスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1.get('id')\n",
    "# 属性の値"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1.attrib\n",
    "# 全属性を表すdict-likeなオブジェクト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1.getparent()\n",
    "# 親要素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile scrape_by_lxml.py\n",
    "\n",
    "import lxml.html\n",
    "\n",
    "# HTMLファイルを読み込みgetroot()メソッドでHtmlElementオブジェクトを得る\n",
    "tree = lxml.html.parse('index.html')\n",
    "html = tree.getroot()\n",
    "\n",
    "# cssselect()メソッドでa要素のリストを取得して、個々のa要素に対して処理を行う\n",
    "for a in html.cssselect('a'):\n",
    "    # href属性とリンクのテキストを取得して表示する\n",
    "    print(a.get('href'), a.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python scrape_by_lxml.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Beautiful Soupによるスクレイピング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('index.html') as f:\n",
    "    soup = BeautifulSoup(f, 'html.parser')\n",
    "# 第1引数にファイルオブジェクトを指定してBeautifulSoupオブジェクトを生成\n",
    "# BeautifulSoupにはファイル名やURLを指定することはできない\n",
    "# 第2引数にパーサーを指定する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# BeautifulSoupのコンストラクターにはHTMLの文字列を渡すことも可能\n",
    "soup = BeautifulSoup('''\n",
    "    <html>\n",
    "    <head><title>八百屋オンライン</title></head>\n",
    "    <body>\n",
    "    <h1 id=\"main\">今日のくだもの</h1>\n",
    "    <ul>\n",
    "        <li>りんご</li>\n",
    "        <li class=\"featured\">みかん</li>\n",
    "        <li>ぶどう</li>\n",
    "    </ul>\n",
    "    </body>\n",
    "    </html>''', 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.h1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(soup.h1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.h1.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.h1.string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(soup.h1.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.ul.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(soup.h1.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.h1['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.h1.get('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.h1.attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.h1.parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.li\n",
    "# 複数の要素がある場合は先頭の要素が取得される"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find('li')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('li')\n",
    "# find_all()メソッドで指定した名前の要素のリストを取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('li', class_='featured')\n",
    "# キーワード引数でclassなどの属性を指定できる\n",
    "# classは予約語なのでclass_を使うことに注意"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all(id='main')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('li', class_='featured')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all(id='main')\n",
    "# タグ名を省略して属性のみで探すことも可能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.select('li')\n",
    "# select()メソッドでCSSセレクターにマッチする要素を取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.select('li.featured')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.select('#main')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile scrape_by_bs4.py\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "with open('index.html') as f:\n",
    "    soup = BeautifulSoup(f, 'html.parser')\n",
    "\n",
    "for a in soup.find_all('a'):\n",
    "    print(a.get('href'), a.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python scrape_by_bs4.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pyqueryによるスクレイピング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyquery import PyQuery as pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = pq(filename='index.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = pq(url='http://example.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = pq('''\n",
    "    <html>\n",
    "    <head><title>八百屋オンライン</title></head>\n",
    "    <body>\n",
    "    <h1 id=\"main\">今日のくだもの</h1>\n",
    "    <ul>\n",
    "        <li>りんご</li>\n",
    "        <li class=\"featured\">みかん</li>\n",
    "        <li>ぶどう</li>\n",
    "    </ul>\n",
    "    </body>\n",
    "    </html>''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d('h1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(d('h1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d('h1')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d('h1').text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d('h1').attr('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d('h1').attr.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d('h1').attr['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d('h1').parent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d('li')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d('li.featured')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d('#main')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d('body').find('li')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d('li').filter('.featured')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d('li').eq(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# RSSのスクレイピング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import feedparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = feedparser.parse('http://b.hatena.ne.jp/hotentry/it.rss')\n",
    "# parse()関数にURLを指定してパースできる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# d = feedparser.parse('it.rss')\n",
    "# parse()関数にはファイルパス、ファイルオブジェクト、XMLの文字列も指定できる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.feed.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d['feed']['title']\n",
    "# dictの形式でもアクセスできる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.feed.link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.feed.description\n",
    "# フィードの説明を取得する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(d.entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.entries[0].title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.entries[0].link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.entries[0].description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.entries[0].updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.entries[0].updated_parsed\n",
    "# 要素の更新日時をパースしてtime.struct_timeを取得する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile scrape_by_feedparser.py\n",
    "\n",
    "import feedparser\n",
    "\n",
    "d = feedparser.parse('http://b.hatena.ne.jp/hotentry/it.rss')\n",
    "\n",
    "for entry in d.entries:\n",
    "    print(entry.link, entry.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python scrape_by_feedparser.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# chapter 3-5 データベースに保存する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MySQLへのデータの保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# データベースとユーザーの作成 > ターミナル操作はEvernoteに"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile save_mysql.py\n",
    "\n",
    "import MySQLdb\n",
    "\n",
    "conn = MySQLdb.connect(db='scraping', user='scraper', passwd='password', charset='utf8mb4')\n",
    "\n",
    "c = conn.cursor()\n",
    "c.execute('DROP TABLE IF EXISTS cities')\n",
    "c.execute('''\n",
    "    CREATE TABLE cities (\n",
    "        rank integer,\n",
    "        city text,\n",
    "        population integer\n",
    "    )\n",
    "''')\n",
    "\n",
    "c.execute('INSERT INTO cities VALUES (%s, %s, %s)', (1, '上海', 24150000))\n",
    "\n",
    "c.execute('INSERT INTO cities VALUES (%(rank)s, %(city)s, %(population)s)',\n",
    "         {'rank': 2, 'city': 'カラチ', 'population': 23500000})\n",
    "\n",
    "c.executemany('INSERT INTO cities VALUES (%(rank)s, %(city)s, %(population)s)', [\n",
    "    {'rank': 3, 'city': '北京', 'population': 21516000},\n",
    "    {'rank': 4, 'city': '天津', 'population': 14722100},\n",
    "    {'rank': 5, 'city': 'イスタンブル', 'population': 14160467},\n",
    "])\n",
    "\n",
    "conn.commit()\n",
    "\n",
    "c.execute('SELECT * FROM cities')\n",
    "for row in c.fetchall():\n",
    "    print(row)\n",
    "    \n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python save_mysql.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MongoDBへのデータの保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 本ではデフォルトのデータベースディレクトリは /data/db 、起動は mongod コマンドのみだが、それではなぜかうまく動かない\n",
    "# ターミナルで mongod --config /usr/local/etc/mongod.conf で起動\n",
    "# 終了は ctrl + c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile save_mongo.py\n",
    "\n",
    "import lxml.html\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# HTMLファイルを読み込み、getroot()メソッドでHtmlElementオブジェクトを得る\n",
    "tree = lxml.html.parse('index.html')\n",
    "html = tree.getroot()\n",
    "\n",
    "client = MongoClient('localhost', 27017)\n",
    "db = client.scraping # scrapingデータベースを取得（作成）する\n",
    "collection = db.links # linksコレクションを取得（作成）する\n",
    "\n",
    "# このスクリプトを何回実行しても同じ結果になるよう、コレクションのドキュメントをすべて削除する\n",
    "collection.delete_many({})\n",
    "\n",
    "# cssselect()メソッドでa要素のリストを取得して、個々のa要素に対して処理を行う\n",
    "for a in html.cssselect('a'):\n",
    "    # href属性とリンクのテキストを取得して保存する\n",
    "    collection.insert_one({\n",
    "        'url': a.get('href'),\n",
    "        'title': a.text,\n",
    "    })\n",
    "\n",
    "# コレクションのすべてのドキュメントを_idの順にソートして取得する\n",
    "for link in collection.find().sort('_id'):\n",
    "    print(link['_id'], link['url'], link['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python save_mongo.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# chapter 3-6 クローラーとURL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 相対URLから絶対URLへの変換例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from urllib.parse import urljoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_url = 'http://example.com/books/top.html'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# // で始まる相対URL\n",
    "urljoin(base_url, '//cdn.example.com/logo.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# / で始まる相対URL\n",
    "urljoin(base_url, '/articles/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ./ 形式の表記\n",
    "urljoin(base_url, './')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# chapter 3-7 Pythonによるクローラーの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile python_crawler_1.py\n",
    "\n",
    "# 一覧ページからURLの一覧を抜き出す(1)\n",
    "\n",
    "import requests\n",
    "import lxml.html\n",
    "\n",
    "response = requests.get('https://gihyo.jp/dp')\n",
    "root = lxml.html.fromstring(response.content)\n",
    "for a in root.cssselect('a[itemprop=\"url\"]'):\n",
    "    url = a.get('href')\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python python_crawler_1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile python_crawler_2.py\n",
    "\n",
    "# 一覧ページからURLの一覧を抜き出す(2)\n",
    "# 不要なリンクを除外し、相対URLを絶対URLに変換する\n",
    "\n",
    "import requests\n",
    "import lxml.html\n",
    "\n",
    "response = requests.get('https://gihyo.jp/dp')\n",
    "root = lxml.html.fromstring(response.content)\n",
    "root.make_links_absolute(response.url) # すべてのリンクを絶対URLに変換する\n",
    "\n",
    "# id=\"listBook\"である要素の子孫のa要素のみを取得する\n",
    "for a in root.cssselect('#listBook a[itemprop=\"url\"]'):\n",
    "    url = a.get('href')\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python python_crawler_2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile python_crawler_3.py\n",
    "\n",
    "# 一覧ページからURLの一覧を抜き出す(3)\n",
    "# あとで利用しやすいよう関数をつかってリファクタリングしておく\n",
    "\n",
    "import requests\n",
    "import lxml.html\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    クローラーのメインの処理\n",
    "    \"\"\"\n",
    "    response = requests.get('https://gihyo.jp/dp')\n",
    "    # scrape_list_page()関数を呼び出し、ジェネレーターイテレーターを取得\n",
    "    urls = scrape_list_page(response)\n",
    "    for url in urls: # ジェネレーターイテレーターはlistなどと同様に繰り返し可能\n",
    "        print(url)\n",
    "\n",
    "def scrape_list_page(response):\n",
    "    \"\"\"\n",
    "    一覧ページのResponseから詳細ページのURLを抜き出すジェネレーター関数\n",
    "    \"\"\"\n",
    "    root = lxml.html.fromstring(response.content)\n",
    "    root.make_links_absolute(response.url) # すべてのリンクを絶対URLに変換する\n",
    "\n",
    "    # id=\"listBook\"である要素の子孫のa要素のみを取得する\n",
    "    for a in root.cssselect('#listBook a[itemprop=\"url\"]'):\n",
    "        url = a.get('href')\n",
    "        yield url # yield文でジェネレーターイテレーターの要素を返す\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python python_crawler_3.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 詳細ページからスクレイピングする(クロール前のテスト)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile python_crawler_4.py\n",
    "\n",
    "# 詳細ページからスクレイピングする(1)\n",
    "\n",
    "import requests\n",
    "import lxml.html\n",
    "\n",
    "\n",
    "def main():\n",
    "    session = requests.Session() # 複数のページをクロールするのでSessionを使う\n",
    "    response = session.get('https://gihyo.jp/dp')\n",
    "    urls = scrape_list_page(response)\n",
    "    for url in urls:\n",
    "        response = session.get(url) # Sessionを使って詳細ページを取得\n",
    "        ebook = scrape_detail_page(response) # 詳細ページからスクレイピングして電子書籍の情報を得る\n",
    "        print(ebook) # 電子書籍の情報を表示\n",
    "        break # まず1ページだけで試すためbreak文でループを抜ける\n",
    "        \n",
    "        \n",
    "def scrape_list_page(response):\n",
    "    root = lxml.html.fromstring(response.content)\n",
    "    root.make_links_absolute(response.url)\n",
    "    \n",
    "    for a in root.cssselect('#listBook a[itemprop=\"url\"]'):\n",
    "        url = a.get('href')\n",
    "        yield url\n",
    "        \n",
    "        \n",
    "def scrape_detail_page(response):\n",
    "    \"\"\"\n",
    "    詳細ページのResponseから電子書籍の情報をdictで取得する\n",
    "    \"\"\"\n",
    "    root = lxml.html.fromstring(response.content)\n",
    "    ebook = {\n",
    "        'url': response.url, # URL\n",
    "        'title': root.cssselect('#bookTitle')[0].text_content(), # タイトル\n",
    "        'price': root.cssselect('.buy')[0].text, # 価格(.textで直接の子である文字列のみを取得)\n",
    "        'content': [h3.text_content() for h3 in root.cssselect('#content > h3')], # 目次\n",
    "    }\n",
    "    return ebook # dictを返す\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python python_crawler_4.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile python_crawler_5.py\n",
    "\n",
    "# 詳細ページからスクレイピングする(2)\n",
    "#  不要な空白や改行は削除したい\n",
    "\n",
    "import re\n",
    "import requests\n",
    "import lxml.html\n",
    "\n",
    "\n",
    "def main():\n",
    "    session = requests.Session() # 複数のページをクロールするのでSessionを使う\n",
    "    response = session.get('https://gihyo.jp/dp')\n",
    "    urls = scrape_list_page(response)\n",
    "    for url in urls:\n",
    "        response = session.get(url) # Sessionを使って詳細ページを取得\n",
    "        ebook = scrape_detail_page(response) # 詳細ページからスクレイピングして電子書籍の情報を得る\n",
    "        print(ebook) # 電子書籍の情報を表示\n",
    "        break # まず1ページだけで試すためbreak文でループを抜ける\n",
    "        \n",
    "        \n",
    "def scrape_list_page(response):\n",
    "    root = lxml.html.fromstring(response.content)\n",
    "    root.make_links_absolute(response.url)\n",
    "    \n",
    "    for a in root.cssselect('#listBook a[itemprop=\"url\"]'):\n",
    "        url = a.get('href')\n",
    "        yield url\n",
    "        \n",
    "        \n",
    "def scrape_detail_page(response):\n",
    "    \"\"\"\n",
    "    詳細ページのResponseから電子書籍の情報をdictで取得する\n",
    "    \"\"\"\n",
    "    root = lxml.html.fromstring(response.content)\n",
    "    ebook = {\n",
    "        'url': response.url, # URL\n",
    "        'title': root.cssselect('#bookTitle')[0].text_content(), # タイトル\n",
    "        'price': root.cssselect('.buy')[0].text.strip(), # 価格(.textで直接の子である文字列のみを取得、strip()で前後の空白を削除)\n",
    "        'content': [normalize_spaces(h3.text_content()) for h3 in root.cssselect('#content > h3')], # 目次\n",
    "    }\n",
    "    return ebook # dictを返す\n",
    "\n",
    "\n",
    "def normalize_spaces(s):\n",
    "    \"\"\"\n",
    "    連続する空白を1つのスペースに置き換え、前後の空白は削除した新しい文字列を取得する\n",
    "    \"\"\"\n",
    "    return re.sub(r'\\s+', ' ', s).strip()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python python_crawler_5.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 詳細ページをクロールする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile python_crawler_6.py\n",
    "\n",
    "# 詳細ページをクロールする\n",
    "#  不要な空白や改行は削除したい\n",
    "# 1秒ごとに電子書籍の情報を取得して表示する\n",
    "\n",
    "import time\n",
    "import re\n",
    "import requests\n",
    "import lxml.html\n",
    "\n",
    "\n",
    "def main():\n",
    "    session = requests.Session() # 複数のページをクロールするのでSessionを使う\n",
    "    response = session.get('https://gihyo.jp/dp')\n",
    "    urls = scrape_list_page(response)\n",
    "    for url in urls:\n",
    "        time.sleep(1) # 1秒のウェイトを入れる\n",
    "        response = session.get(url) # Sessionを使って詳細ページを取得\n",
    "        ebook = scrape_detail_page(response) # 詳細ページからスクレイピングして電子書籍の情報を得る\n",
    "        print(ebook) # 電子書籍の情報を表示\n",
    "        \n",
    "        \n",
    "def scrape_list_page(response):\n",
    "    root = lxml.html.fromstring(response.content)\n",
    "    root.make_links_absolute(response.url)\n",
    "    \n",
    "    for a in root.cssselect('#listBook a[itemprop=\"url\"]'):\n",
    "        url = a.get('href')\n",
    "        yield url\n",
    "        \n",
    "        \n",
    "def scrape_detail_page(response):\n",
    "    \"\"\"\n",
    "    詳細ページのResponseから電子書籍の情報をdictで取得する\n",
    "    \"\"\"\n",
    "    root = lxml.html.fromstring(response.content)\n",
    "    ebook = {\n",
    "        'url': response.url, # URL\n",
    "        'title': root.cssselect('#bookTitle')[0].text_content(), # タイトル\n",
    "        'price': root.cssselect('.buy')[0].text.strip(), # 価格(.textで直接の子である文字列のみを取得、strip()で前後の空白を削除)\n",
    "        'content': [normalize_spaces(h3.text_content()) for h3 in root.cssselect('#content > h3')], # 目次\n",
    "    }\n",
    "    return ebook # dictを返す\n",
    "\n",
    "\n",
    "def normalize_spaces(s):\n",
    "    \"\"\"\n",
    "    連続する空白を1つのスペースに置き換え、前後の空白は削除した新しい文字列を取得する\n",
    "    \"\"\"\n",
    "    return re.sub(r'\\s+', ' ', s).strip()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python python_crawler_6.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# スクレイピングしたデータを保存する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile python_crawler_final.py\n",
    "\n",
    "# 詳細ページをクロールする\n",
    "#  不要な空白や改行は削除したい\n",
    "# 1秒ごとに電子書籍の情報を取得して表示する\n",
    "# 取得したデータをMongoDBに保存する(あらかじめMongoDBを起動しておく)\n",
    "# mongod --config /usr/local/etc/mongod.conf\n",
    "# 2回目以降はクロール済みのURLはクロールしないようにする\n",
    "\n",
    "import time\n",
    "import re\n",
    "import requests\n",
    "import lxml.html\n",
    "from pymongo import MongoClient\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    クローラーのメインの処理\n",
    "    \"\"\"\n",
    "    \n",
    "    client = MongoClient('localhost', 27017) # ローカルホストのMongoDBに接続する\n",
    "    collection = client.scraping.ebooks # scrapingデータベースのebooksコレクションを得る\n",
    "    # データを一意に識別するキーを格納するkeyフィールドにユニークなインデックスを作成する\n",
    "    collection.create_index('key', unique=True)\n",
    "    \n",
    "    response = requests.get('https://gihyo.jp/dp') # 一覧ページを取得する\n",
    "    urls = scrape_list_page(response) # 詳細ページのURL一覧を得る\n",
    "    for url in urls:\n",
    "        key = extract_key(url) # URLからキーを取得する\n",
    "        \n",
    "        ebook = collection.find_one({'key': key}) # MongoDBからkeyに該当するデータを探す\n",
    "        if not ebook: # MongoDBに存在しない場合だけ、詳細ページをクロールする\n",
    "            time.sleep(1) # 1秒のウェイトを入れる\n",
    "            response = requests.get(url) # 詳細ページを取得\n",
    "            ebook = scrape_detail_page(response) # 詳細ページからスクレイピングして電子書籍の情報を得る\n",
    "            collection.insert_one(ebook) # 電子書籍の情報をMongoDBに保存する\n",
    "            \n",
    "        print(ebook) # 電子書籍の情報を表示\n",
    "        \n",
    "        \n",
    "def scrape_list_page(response):\n",
    "    \"\"\"\n",
    "    一覧ページのResponseから詳細ページのURLを抜き出す\n",
    "    \"\"\"\n",
    "    root = lxml.html.fromstring(response.content)\n",
    "    root.make_links_absolute(response.url)\n",
    "    \n",
    "    for a in root.cssselect('#listBook a[itemprop=\"url\"]'):\n",
    "        url = a.get('href')\n",
    "        yield url\n",
    "        \n",
    "        \n",
    "def scrape_detail_page(response):\n",
    "    \"\"\"\n",
    "    詳細ページのResponseから電子書籍の情報をdictで得る\n",
    "    \"\"\"\n",
    "    root = lxml.html.fromstring(response.content)\n",
    "    ebook = {\n",
    "        'url': response.url, # URL\n",
    "        'key': extract_key(response.url), # URLから抜き出したキー\n",
    "        'title': root.cssselect('#bookTitle')[0].text_content(), # タイトル\n",
    "        'price': root.cssselect('.buy')[0].text.strip(), # 価格(.textで直接の子である文字列のみを取得、strip()で前後の空白を削除)\n",
    "        'content': [normalize_spaces(h3.text_content()) for h3 in root.cssselect('#content > h3')], # 目次\n",
    "    }\n",
    "    return ebook # dictを返す\n",
    "\n",
    "\n",
    "def extract_key(url):\n",
    "    \"\"\"\n",
    "    URLからキー(URLの末尾のISBN)を抜き出す\n",
    "    \"\"\"\n",
    "    m = re.search(r'/([^/]+)$', url)\n",
    "    return m.group(1)\n",
    "\n",
    "\n",
    "def normalize_spaces(s):\n",
    "    \"\"\"\n",
    "    連続する空白を1つのスペースに置き換え、前後の空白は削除した新しい文字列を取得する\n",
    "    \"\"\"\n",
    "    return re.sub(r'\\s+', ' ', s).strip()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python python_crawler_final.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
