{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# chapter 6-1 Scrapyの概要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pip install scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# code 6.1 Scrapinghub社のブログから投稿のタイトルを取得するSpider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting myspider.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile myspider.py\n",
    "\n",
    "# これはScrapyのWebサイト（https://scrapy.org/）に掲載されているサンプルコードにコメントを追加したもの\n",
    "# Scrapinghub社のブログから投稿のタイトルを取得するSpider\n",
    "\n",
    "import scrapy\n",
    "\n",
    "class BlogSpider(scrapy.Spider):\n",
    "    name = 'blogspider' # Spiderの名前\n",
    "    # クロールを開始するURLのリスト\n",
    "    start_urls = ['https://blog.scrapinghub.com']\n",
    "    \n",
    "    def parse(self, response):\n",
    "        \"\"\"\n",
    "        トップページからカテゴリページへのリンクを抜き出してたどる\n",
    "        \"\"\"\n",
    "        for url in response.css('ul li a::attr(\"href\")').ref('.*/category/.*'):\n",
    "            yield scrapy.Reauest(response.urljoin(url), self.parse_titles)\n",
    "            \n",
    "            \n",
    "    def parse_titles(self, response):\n",
    "        \"\"\"\n",
    "        カテゴリページからそのカテゴリの投稿のタイトルをすべて抜き出す\n",
    "        \"\"\"\n",
    "        for post_title in response.css('div.entries > ul > li a::text').extract():\n",
    "            yield {'title': post_title}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing myspider_mod.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile myspider_mod.py\n",
    "\n",
    "# これはScrapyのWebサイト（https://scrapy.org/）に掲載されているサンプルコードにコメントを追加したもの\n",
    "# ScrapingHub社のブログの構造の変更により，リスト6.1の`myspider.py`は投稿を1件も取得できなくなった\n",
    "# ScrapyのWebサイトに掲載されているサンプルコードも変更されている\n",
    "# ブログの構造の変化に合わせて'`myspider.py`の時点のサンプルコードから変更\n",
    "\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class BlogSpider(scrapy.Spider):\n",
    "    name = 'blogspider'  # Spiderの名前。\n",
    "    # クロールを開始するURLのリスト。\n",
    "    start_urls = ['https://blog.scrapinghub.com']\n",
    "\n",
    "    def parse(self, response):\n",
    "        \"\"\"\n",
    "        ページから投稿のタイトルをすべて抜き出し、ベージャーをたどる。\n",
    "        \"\"\"\n",
    "        # ページから投稿のタイトルをすべて抜き出す。\n",
    "        for title in response.css('h2.entry-title'):\n",
    "            yield {'title': title.css('a ::text').extract_first()}\n",
    "\n",
    "        # ページャーの次のページへのリンクを取得し、次のページがあればたどる。\n",
    "        # 次のページもparse()メソッドで処理する。\n",
    "        next_page = response.css('div.prev-post > a ::attr(href)').extract_first()\n",
    "        if next_page:\n",
    "            yield scrapy.Request(response.urljoin(next_page), callback=self.parse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このSpiderは「一覧のみパターン」のSpiderで，まずブログのトップページから投稿のタイトルをすべて抜き出します。\n",
    "続いてページ下部にある「OLDER POST」というリンクをたどって，次のページを取得します。\n",
    "そこからも同様に投稿のタイトルをすべて抜き出し，さらに次のページへと再帰的に繰り返します。\n",
    "後のページまでたどって「OLDER POST」というリンクがなくなったら終了です。\n",
    "実行結果は書籍内のものと変わりません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!scrapy runspider myspider_mod.py -o items.jl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!cat items.jl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# chapter 6-2 Spiderの作成と実行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapyプロジェクトの開始"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Scrapy project 'myproject', using template directory '/Users/Really/.pyenv/versions/anaconda3-4.3.1/lib/python3.6/site-packages/scrapy/templates/project', created in:\r\n",
      "    /Users/Really/Python_ScrapingBook/myproject\r\n",
      "\r\n",
      "You can start your first spider with:\r\n",
      "    cd myproject\r\n",
      "    scrapy genspider example example.com\r\n"
     ]
    }
   ],
   "source": [
    "!scrapy startproject myproject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "myproject\r\n",
      "├── myproject\r\n",
      "│   ├── __init__.py\r\n",
      "│   ├── __pycache__\r\n",
      "│   ├── items.py\r\n",
      "│   ├── middlewares.py\r\n",
      "│   ├── pipelines.py\r\n",
      "│   ├── settings.py\r\n",
      "│   └── spiders\r\n",
      "│       ├── __init__.py\r\n",
      "│       └── __pycache__\r\n",
      "└── scrapy.cfg\r\n",
      "\r\n",
      "4 directories, 7 files\r\n"
     ]
    }
   ],
   "source": [
    "!tree myproject"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Webサイトに高負荷をかけないよう、最低限settings.pyに次の1行を追記する\n",
    "# (これを設定しないとダウンロード間隔は0秒になる)\n",
    "# DOWNLOAD_DELAY = 1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# プロジェクトのディレクトリに移動する\n",
    "# 今後コマンドを実行する際は基本的にscrapy.cfgがあるこのディレクトリで実行する\n",
    "\n",
    "cd myproject\n",
    "\n",
    "# ところがJupyter Notebook内でディレクトリを移動する方法がわからない。。\n",
    "# 移動できない?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Itemの作成"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Itemは、Spiderが抜き出したデータを格納しておくためのオブジェクト\n",
    "# プロジェクトのitems.pyに定義する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# code 6.2 ニュースのヘッドラインを格納するためのItem"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# items.pyに以下のクラスを追加する\n",
    "# ニュースのヘッドラインを格納するためのItem\n",
    "# scrapy.Itemを継承し、フィールドを<フィールド名> = scrapy.Field()という形で定義する\n",
    "\n",
    "class Headline(scrapy.Item):\n",
    "    \"\"\"\n",
    "    ニュースのヘッドラインを表すItem\n",
    "    \"\"\"\n",
    "    \n",
    "    title = scrapy.Field()\n",
    "    body = scrapy.Field()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spiderの作成"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# シェルで、myproject内で以下を実行\n",
    "scrapy genspider news news.yahoo.co.jp"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# spidersディレクトリ内にnews.pyが作成される"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapyにより生成されたSpider\n",
    "# \"news.py\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class NewsSpider(scrapy.Spider):\n",
    "    name = 'news'\n",
    "    allowed_domains = ['news.yahoo.co.jp']\n",
    "    start_urls = ['http://news.yahoo.co.jp/']\n",
    "\n",
    "    def parse(self, response):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# code 6.3 トピックスのリンクのURLを表示するSpider\n",
    "# \"news.py\"\n",
    "(上記ファイルに変更を加えたもの。1行目のエンコーディング宣言はPython2向けのものなのでPython3では不要)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import scrapy\n",
    "\n",
    "\n",
    "class NewsSpider(scrapy.Spider):\n",
    "    name = 'news' # Spiderの名前\n",
    "    # クロール対象とするドメインのリスト\n",
    "    allowed_domains = ['news.yahoo.co.jp']\n",
    "    # クロールを開始するURLのリスト\n",
    "    # 1要素のタプルの末尾にはカンマが必要\n",
    "    start_urls = (\n",
    "        'http://news.yahoo.co.jp/',\n",
    "    )\n",
    "\n",
    "    def parse(self, response):\n",
    "        \"\"\"\n",
    "        トップページのトピックス一覧から個々のトピックスへのリンクを抜き出して表示する\n",
    "        \"\"\"\n",
    "        print(response.css('ul.topics a::attr(\"href\")').extract())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# シェルで、myproject内で以下を実行\n",
    "scrapy crawl news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# code 6.4 抜き出したトピックスのリンクをたどるSpider\n",
    "# \"news.py\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import scrapy\n",
    "\n",
    "\n",
    "class NewsSpider(scrapy.Spider):\n",
    "    name = 'news' # Spiderの名前\n",
    "    # クロール対象とするドメインのリスト\n",
    "    allowed_domains = ['news.yahoo.co.jp']\n",
    "    # クロールを開始するURLのリスト\n",
    "    # 1要素のタプルの末尾にはカンマが必要\n",
    "    start_urls = (\n",
    "        'http://news.yahoo.co.jp/',\n",
    "    )\n",
    "\n",
    "    def parse(self, response):\n",
    "        \"\"\"\n",
    "        トップページのトピックス一覧から個々のトピックスへのリンクを抜き出してたどる\n",
    "        \"\"\"\n",
    "        for url in response.css('ul.topics a::attr(\"href\")').re(r'/pickup/\\d+$'):\n",
    "            yield scrapy.Request(response.urljoin(url), self.parse_topics)\n",
    "\n",
    "    def parse_topics(self, response):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapy Shellによるインタラクティブなスクレイピング"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Scrapy Shellを起動する"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# CSSセレクターやXpathでノードを修復する"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# ヘッドラインのタイトルと本文を取得する"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 以上(シェルで実行。書籍参照)を元にNewsSpiderのparse_topics()メソッドを実装する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# code 6.5 Yahoo!ニュースからトピックスを抽出するSpider\n",
    "# \"news.py\" 最終形"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import scrapy\n",
    "\n",
    "from myproject.items import Headline # ItemのHeadlineクラスをインポート\n",
    "\n",
    "\n",
    "class NewsSpider(scrapy.Spider):\n",
    "    name = 'news' # Spiderの名前\n",
    "    # クロール対象とするドメインのリスト\n",
    "    allowed_domains = ['news.yahoo.co.jp']\n",
    "    # クロールを開始するURLのリスト\n",
    "    # 1要素のタプルの末尾にはカンマが必要\n",
    "    start_urls = (\n",
    "        'http://news.yahoo.co.jp/',\n",
    "    )\n",
    "\n",
    "    def parse(self, response):\n",
    "        \"\"\"\n",
    "        トップページのトピックス一覧から個々のトピックスへのリンクを抜き出してたどる\n",
    "        \"\"\"\n",
    "        for url in response.css('ul.topics a::attr(\"href\")').re(r'/pickup/\\d+$'):\n",
    "            yield scrapy.Request(response.urljoin(url), self.parse_topics)\n",
    "\n",
    "    def parse_topics(self, response):\n",
    "        \"\"\"\n",
    "        トピックスのページからタイトルと本文を抜き出す\n",
    "        \"\"\"\n",
    "        item = Headline() # Headlineオブジェクトを作成\n",
    "        item['title'] = response.css('.newsTitle ::text').extract_first() # タイトル\n",
    "        item['body'] = response.css('.hbody').xpath('string()').extract_first() # 本文\n",
    "        yield item # Itemをyieldして、データを抽出する\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 作成したSpiderの実行"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Spiderを実行する"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# スクレイピングしたデータの出力"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 実行の流れ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# chapter 6-3 実践的なクローリング"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# クローリングでリンクをたどる"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# CrawlSpiderを作る"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# code 6.6 Yahoo!ニュースからトピックスを抽出するCrawlSpider\n",
    "# \"news_crawl.py\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from scrapy.spiders import CrawlSpider, Rule\n",
    "from scrapy.linkextractors import LinkExtractor\n",
    "\n",
    "from myproject.items import Headline # ItemのHeadlineクラスをインポート\n",
    "\n",
    "\n",
    "class NewsCrawlSpider(CrawlSpider):\n",
    "    name = 'news_crawl' # Spiderの名前\n",
    "    # クロール対象とするドメインのリスト\n",
    "    allowed_domains = ['news.yahoo.co.jp']\n",
    "    # クロールを開始するURLのリスト\n",
    "    # 1要素のタプルの末尾にはカンマが必要\n",
    "    start_urls = (\n",
    "        'http://news.yahoo.co.jp/',\n",
    "    )\n",
    "\n",
    "    # リンクをたどるためのルールのリスト\n",
    "    rules = (\n",
    "        # トピックスのページへのリンクをたどり、レスポンスをparse_topics()メソッドで処理する\n",
    "        Rule(LinkExtractor(allow=r'/pickup/\\d+$'), callback='parse_topics'),\n",
    "    )\n",
    "\n",
    "    def parse_topics(self, response):\n",
    "        \"\"\"\n",
    "        トピックスのページからタイトルと本文を抜き出す\n",
    "        \"\"\"\n",
    "        item = Headline() # Headlineオブジェクトを作成\n",
    "        item['title'] = response.css('.newsTitle ::text').extract_first() # タイトル\n",
    "        item['body'] = response.css('.hbody').xpath('string()').extract_first() # 本文\n",
    "        yield item # Itemをyieldして、データを抽出する\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XMLサイトマップを使ったクローリング"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# XMLサイトマップからWebサイトの構造を把握する"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# SitemapSpiderの作成と実行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# code 6.7 WIRED.jpをクロールするSitemapSpider\n",
    "# \"wiredjp.py\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from scrapy.spiders import SitemapSpider\n",
    "\n",
    "\n",
    "class WiredjpSpider(SitemapSpider):\n",
    "    name = \"wiredjp\"\n",
    "    allowed_domains = [\"wired.jp\"]\n",
    "\n",
    "    # XMLサイトマップのURLのリスト\n",
    "    # robots.txtのURLを指定すると、SitemapディレクティブからXMLサイトマップのURLを取得する\n",
    "    # WIRED.jpの仕様が変更されたらしく、robots.txt内にXMLサイトマップのURLの記述が無くなっている\n",
    "    sitemap_urls = [\n",
    "        'https://wired.jp/sitemap.xml', # http://wired.jp/robots.txt\n",
    "    ]\n",
    "    # サイトマップインデックスからたどるサイトマップURLの正規表現のリスト\n",
    "    # このリストの正規表現にマッチするURLのサイトマップのみをたどる\n",
    "    # sitemap_followを指定しない場合は、すべてのサイトマップをたどる\n",
    "    sitemap_follow = [\n",
    "        r'post-2017-',\n",
    "    ]\n",
    "    # サイトマップに含まれるURLを処理するコールバック関数を指定するルールのリスト\n",
    "    # ルールは(正規表現、正規表現にマッチするURLを処理するコールバック関数)という2要素のタプルで指定する\n",
    "    # sitemap_rulesを指定しない場合はすべてのURLのコールバック関数はparseメソッドとなる\n",
    "    sitemap_rules = [\n",
    "        (r'/2017/\\d\\d/\\d\\d/', 'parse_post'),\n",
    "    ]\n",
    "\n",
    "    def parse_post(self, response):\n",
    "        # 詳細ページから投稿のタイトルを抜き出す\n",
    "        yield {\n",
    "            'title': response.css('h1.post-title::text').extract_first(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# chapter 6-4 抜き出したデータの処理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Item Pipelineの概要"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Pipelineの作成"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Pipelineの使用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データの検証"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MongoDBへのデータの保存"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MySQLへのデータの保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# chapter 6-5 Scrapyの設定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 設定の方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# クロール先に迷惑をかけないための設定項目"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 並行処理に関する設定項目"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HTTPリクエストに関する設定項目"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HTTPキャッシュの設定項目"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# エラー処理に関する設定項目"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# プロキシを使用する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# chapter 6-6 Scrapyの拡張"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ダウロード処理を拡張する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spiderの挙動を拡張する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# chapter 6-7 クローリングによるデータの収集と活用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# レストラン情報の収集"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 食べログのサイトの構造"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://tabelog.com/tokyo/rstLst/lunch/?LstCos=0&LstCosT=2&RdoCosTp=1&LstSitu=0&ChkCoupon=0&ChkCampaign=0\n",
    "# 値が0のパラメーターは省略しても結果が変わらない\n",
    "# https://tabelog.com/tokyo/rstLst/lunch/?LstCosT=2&RdoCosTp=1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 詳細ページからデータを抜き出す"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "scrapy shell https://tabelog.com/tokyo/A1309/A130902/13000852/ # 詳細ページ\n",
    "response.css('.display-name').xpath('string()').extract_first().strip() # 店名\n",
    "response.css('[class=\"rstinfo-table__address\"]').xpath('string()').extract_first() # 住所、自分でアレンジ\n",
    "response.css('img.js-map-lazyload::attr(\"data-original\")').extract_first() # Google Static Mapの画像のURL\n",
    "response.css('img.js-map-lazyload::attr(\"data-original\")').re(r'markers=.*?%7C([\\d.]+),([\\d.]+)') # 緯度と経度\n",
    "response.css('dt:contains(\"最寄り駅\")+dd span::text').extract_first() # 最寄り駅\n",
    "response.css('[rel=\"v:rating\"] span::text').extract_first() # スコア\n",
    "\n",
    "response.css('[rel=\"v:rating\"]').xpath('string()').extract_first().strip() # スコア、自分でアレンジ"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Spiderを作成する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
